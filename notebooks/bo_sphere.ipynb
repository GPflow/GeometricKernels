{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Optimization on a Sphere Manifold $\\mathbb{S}^2$\n",
    "\n",
    "This notebooks illustrates the application of Bayesian optimization (BO) on a sphere manifold. \n",
    "To run it, you need to have [BoTorch](https://botorch.org/) installed.\n",
    "\n",
    "References:\n",
    "\n",
    "[1] N. Jaquier, V. Borovitskiy, A. Smolensky, A. Terenin, T. Asfour, and L. Rozo. Geometry-aware Bayesian Optimization in Robotics using Riemannian Matérn Kernels. In Conference on Robot Learning (CoRL), 2021. https://arxiv.org/pdf/2111.01460.pdf\n",
    "\n",
    "[2] N. Jaquier, L. Rozo, S. Calinon, and M. Bürger. Bayesian Optimization Meets Riemannian Manifolds in Robot Learning. In Conference on Robot Learning (CoRL), 2019. https://arxiv.org/pdf/1910.04998.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import gpytorch\n",
    "import botorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using numpy backend\n"
     ]
    }
   ],
   "source": [
    "import geometric_kernels.torch\n",
    "from geometric_kernels.spaces.hypersphere import Hypersphere\n",
    "from geometric_kernels.kernels.geometric_kernels import MaternKarhunenLoeveKernel\n",
    "from geometric_kernels.frontends.pytorch.gpytorch import GPytorchGeometricKernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first set the numpy and pytorch seeds for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize a sphere manifold $\\mathbb{S}^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = 3\n",
    "hypersphere = Hypersphere(dim=dimension-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the function to optimize. \n",
    "\n",
    "Here we use the Ackley function (see, e.g., https://www.sfu.ca/~ssurjano/ackley.html).\n",
    "The function is defined on the tangent space of the base point $(1, 0, 0, ...)$ and projected to the manifold via the exponential map. The value of the function is therefore computed by projecting the point on the manifold to the tangent space of the base point and by computing the value of the Ackley function in this Euclidean space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ackley_function_sphere(x):\n",
    "    # Data to numpy\n",
    "    torch_type = x.dtype\n",
    "    x = x.cpu().detach().numpy()\n",
    "    if np.ndim(x) < 2:\n",
    "        x = x[None]\n",
    "\n",
    "    # Dimension of the manifold\n",
    "    dimension = x.shape[-1]\n",
    "\n",
    "    # Projection in tangent space of the mean.\n",
    "    # The base is fixed at (1, 0, 0, ...) for simplicity. Therefore, the tangent plane is aligned with the axis x.\n",
    "    # The first coordinate of x_proj is always 0, so that vectors in the tangent space can be expressed in a\n",
    "    # dimension-1 dimensional space by simply ignoring the first coordinate.\n",
    "    base = np.zeros((1, dimension))\n",
    "    base[0, 0] = 1.\n",
    "    x_proj = hypersphere.to_tangent(x, base)[0]\n",
    "\n",
    "    # Remove first dim\n",
    "    x_proj_red = x_proj[1:]\n",
    "    reduced_dimension = dimension-1\n",
    "\n",
    "    # Ackley function parameters\n",
    "    a = 20\n",
    "    b = 0.2\n",
    "    c = 2*np.pi\n",
    "\n",
    "    # Ackley function\n",
    "    aexp_term = -a * np.exp(-b * np.sqrt(np.sum(x_proj_red**2) / reduced_dimension))\n",
    "    expcos_term = - np.exp( np.sum(np.cos(c*x_proj_red) / reduced_dimension))\n",
    "    y = aexp_term + expcos_term + a + np.exp(1.)\n",
    "\n",
    "    return torch.tensor(y[None, None], dtype=torch_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate 5 random data locations on the sphere to be used as initial design for BO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: tensor([[ 0.2453, -0.6197,  0.7455],\n",
      "        [-0.2639, -0.6081,  0.7487],\n",
      "        [ 0.8036, -0.5950,  0.0147],\n",
      "        [-0.8280,  0.4246,  0.3662],\n",
      "        [ 0.4219, -0.8945, -0.1478]], dtype=torch.float64)\n",
      "Outputs:  tensor([4.5962, 4.5936, 3.2448, 3.7839, 3.1244], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "nb_data_init = 5\n",
    "\n",
    "x_data = torch.tensor(hypersphere.random_point(nb_data_init))\n",
    "y_data = torch.zeros(nb_data_init, dtype=torch.float64)\n",
    "for n in range(nb_data_init):\n",
    "    y_data[n] = ackley_function_sphere(x_data[n])\n",
    "\n",
    "print('Inputs:', x_data)\n",
    "print('Outputs: ', y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the BO surrogate model\n",
    "Here we use a Gaussian process with a Matérn kernel on $\\mathbb{S}^2$, as by [1].\n",
    "\n",
    "The kernel has three positive real hyperparameters: vairance $\\sigma^2$, length scale $\\kappa$ and smoothness $\\nu$.\n",
    "Definiting a Gaussian process model also requires a liklihood noise hyperparameter $\\sigma_n^2$.\n",
    "\n",
    "We use the $\\operatorname{Gamma}(2.0, 0.15)$ prior for $\\sigma$, the squre root of $\\sigma^2$, and $\\operatorname{Gamma}(1.1, 0.05)$ prior for $\\sigma_n^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of eigenfunctions requested does not lead to complete levels of spherical harmonics. We have thus increased the number to 16, which includes all spherical harmonics up to degree 4 (excl.)\n",
      "The number of eigenfunctions requested does not lead to complete levels of spherical harmonics. We have thus increased the number to 16, which includes all spherical harmonics up to degree 4 (excl.)\n"
     ]
    }
   ],
   "source": [
    "_TRUNCATION_LEVEL = 10\n",
    "base_kernel = GPytorchGeometricKernel(MaternKarhunenLoeveKernel(hypersphere, _TRUNCATION_LEVEL))\n",
    "kernel = gpytorch.kernels.ScaleKernel(base_kernel,\n",
    "                                      outputscale_prior=gpytorch.priors.torch_priors.GammaPrior(2.0, 0.15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define the likelihood of the Gaussian process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_prior = gpytorch.priors.torch_priors.GammaPrior(1.1, 0.05)\n",
    "noise_prior_mode = (noise_prior.concentration - 1) / noise_prior.rate\n",
    "lik_fct = gpytorch.likelihoods.gaussian_likelihood.GaussianLikelihood(noise_prior=noise_prior,\n",
    "                                                                      noise_constraint=\n",
    "                                                                      gpytorch.constraints.GreaterThan(1e-8),\n",
    "                                                                      initial_value=noise_prior_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally initialize the GP model, as well as the marginal likelihood function that will be used to optimize its parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = botorch.models.SingleTaskGP(x_data, y_data[:, None], covar_module=kernel, likelihood=lik_fct)\n",
    "mll_fct = gpytorch.mlls.ExactMarginalLogLikelihood(model.likelihood, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian optimization loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_best_f, index = y_data.min(0)\n",
    "best_x = [x_data[index]]\n",
    "best_f = [new_best_f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the bounds for the optimization of the acquisition function, as well as the constraints that must be satisfied by candidate points on the sphere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = torch.stack([-torch.ones(dimension, dtype=torch.float64), torch.ones(dimension, dtype=torch.float64)])\n",
    "\n",
    "def upper_constraint(x):\n",
    "    return 1.0 - torch.linalg.norm(x, dim=-1)\n",
    "\n",
    "def lower_constraint(x):\n",
    "    return torch.linalg.norm(x, dim=-1) - 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run our BO loop. At each iteration, we first optimize our GP model, then we define the acquisition function, and select the new candidate point as the point maximizing the acquisition function. \n",
    "\n",
    "Notice that, for the sake of simplicity, we here use a constrained optimization on the sphere to guarantee that our new candidate point belongs to the sphere manifold. However, for a fully geometry-aware BO loop, Riemannian optimization should be used to optimize the acquisition function on the manifold, see [1], [2] for details, and https://github.com/NoemieJaquier/MaternGaBO for an implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vabor112/.local/lib/python3.8/site-packages/gpytorch/lazy/lazy_tensor.py:1810: UserWarning: torch.triangular_solve is deprecated in favor of torch.linalg.solve_triangularand will be removed in a future PyTorch release.\n",
      "torch.linalg.solve_triangular has its arguments reversed and does not return a copy of one of the inputs.\n",
      "X = torch.triangular_solve(B, A).solution\n",
      "should be replaced with\n",
      "X = torch.linalg.solve_triangular(A, B). (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:1672.)\n",
      "  Linv = torch.triangular_solve(Eye, L, upper=False).solution\n",
      "/home/vabor112/.local/lib/python3.8/site-packages/gpytorch/distributions/multivariate_normal.py:259: NumericalWarning: Negative variance values detected. This is likely due to numerical instabilities. Rounding negative variances up to 1e-10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\t Best f 3.124422125710016\n"
     ]
    }
   ],
   "source": [
    "n_iters = 25\n",
    "for iteration in range(n_iters):\n",
    "    # Fit GP model\n",
    "    botorch.fit_gpytorch_model(mll=mll_fct)\n",
    "\n",
    "    # Define the acquisition function\n",
    "    acq_fct = botorch.acquisition.ExpectedImprovement(model=model, best_f=best_f[-1], maximize=False)\n",
    "    \n",
    "    # Initial conditions to optimize the acquisition function\n",
    "    batch_initial_conditions = torch.tensor(hypersphere.random_point(100))\n",
    "    batch_initial_conditions /= torch.linalg.norm(batch_initial_conditions, dim=-1)[:, None]\n",
    "\n",
    "\n",
    "    # Get new candidate\n",
    "    new_x, acq_new_x = botorch.optim.optimize_acqf(acq_fct, bounds=bounds, q=1, num_restarts=5, raw_samples=100,\n",
    "                                                       nonlinear_inequality_constraints=[upper_constraint,\n",
    "                                                                                         lower_constraint],\n",
    "                                                       batch_initial_conditions=batch_initial_conditions[:, None])\n",
    "    # Get new observation\n",
    "    new_y = ackley_function_sphere(new_x)[0]\n",
    "\n",
    "    # Update training points\n",
    "    x_data = torch.cat((x_data, new_x))\n",
    "    y_data = torch.cat((y_data, new_y))\n",
    "\n",
    "    # Update best observation\n",
    "    new_best_f, index = y_data.min(0)\n",
    "    best_x.append(x_data[index])\n",
    "    best_f.append(new_best_f)\n",
    "\n",
    "    # Update the model\n",
    "    model.set_train_data(x_data, y_data, strict=False)  # strict False necessary to add datapoints\n",
    "\n",
    "    print(\"Iteration \" + str(iteration) + \"\\t Best f \" + str(new_best_f.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_eval = x_data.cpu().numpy()\n",
    "y_eval = y_data.cpu().numpy()[:, None]\n",
    "best_x_np = np.array([x.cpu().detach().numpy() for x in best_x])\n",
    "best_f_np = np.array([f.cpu().detach().numpy() for f in best_f])[:, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pl\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the true minimum value of the objective function to evaluate the performance of the BO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_min_x = np.zeros((1, dimension))\n",
    "true_min_x[0, 0] = 1.\n",
    "true_min_value = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We display the candidate points on the sphere evaluated in the BO loop. Their color indicate the value of the function (yellow is low, purple is high). The best candidate is displayed as a red diamond, and the true minimum as a blue cross."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "# Make the panes transparent\n",
    "ax.xaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n",
    "ax.yaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n",
    "ax.zaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n",
    "\n",
    "# Make the grid lines transparent\n",
    "ax.xaxis._axinfo[\"grid\"]['color'] = (1, 1, 1, 0)\n",
    "ax.yaxis._axinfo[\"grid\"]['color'] = (1, 1, 1, 0)\n",
    "ax.zaxis._axinfo[\"grid\"]['color'] = (1, 1, 1, 0)\n",
    "\n",
    "# Remove axis\n",
    "ax._axis3don = False\n",
    "\n",
    "# Initial view\n",
    "# ax.view_init(elev=10, azim=-20.)  # (default: elev=30, azim=-60)\n",
    "ax.view_init(elev=10, azim=30.)  # (default: elev=30, azim=-60)\n",
    "\n",
    "# Plot sphere\n",
    "n_elems = 100\n",
    "r = 0.99\n",
    "u = np.linspace(0, 2 * np.pi, n_elems)\n",
    "v = np.linspace(0, np.pi, n_elems)\n",
    "\n",
    "x = r * np.outer(np.cos(u), np.sin(v))\n",
    "y = r * np.outer(np.sin(u), np.sin(v))\n",
    "z = r * np.outer(np.ones(np.size(u)), np.cos(v))\n",
    "\n",
    "ax.plot_surface(x, y, z, rstride=4, cstride=4, color=[0.8, 0.8, 0.8], linewidth=0, alpha=0.4)\n",
    "\n",
    "lim = 1.1\n",
    "ax.set_xlim3d([-lim, lim])\n",
    "ax.set_ylim3d([-lim, lim])\n",
    "ax.set_zlim3d([-0.75*lim, 0.75*lim])\n",
    "\n",
    "# Plot evaluated points\n",
    "max_colors = np.max(y_eval - true_min_value)\n",
    "for n in range(x_eval.shape[0]):\n",
    "    ax.scatter(x_eval[n, 0], x_eval[n, 1], x_eval[n, 2], s=50,\n",
    "               c=pl.cm.inferno(1. - (y_eval[n] - true_min_value) / max_colors))\n",
    "\n",
    "# Plot true minimum\n",
    "ax.scatter(true_min_x[0, 0], true_min_x[0, 1], true_min_x[0, 2], s=200, c='b', marker='P')\n",
    "\n",
    "# Plot BO minimum\n",
    "ax.scatter(best_x_np[-1][0], best_x_np[-1][1], best_x_np[-1][2], s=100, c='r', marker='D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally display the distance between consecutive candidate points on the sphere, and the convergence plot of our BO algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sphere_distance(x, y):\n",
    "    if np.ndim(x) < 2:\n",
    "        x = x[:, None]\n",
    "    if np.ndim(y) < 2:\n",
    "        y = y[:, None]\n",
    "\n",
    "    # Compute the inner product (should be [-1,1])\n",
    "    inner_product = np.dot(x.T, y)\n",
    "    inner_product = np.max(np.min(inner_product, 1), -1)\n",
    "    return np.arccos(inner_product)\n",
    "\n",
    "# Compute distances between consecutive x's and best evaluation for each iteration\n",
    "neval = x_eval.shape[0]\n",
    "distances = np.zeros(neval-1)\n",
    "for n in range(neval-1):\n",
    "    distances[n] = sphere_distance(x_eval[n + 1, :], x_eval[n, :])\n",
    "\n",
    "Y_best = np.ones(neval)\n",
    "for i in range(neval):\n",
    "    Y_best[i] = y_eval[:(i + 1)].min()\n",
    "\n",
    "# Plot distances between consecutive x's\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(np.array(range(neval - 1)), distances, '-ro')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('d(x[n], x[n-1])')\n",
    "plt.title('Distance between consecutive x\\'s')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot value of the best candidate\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(np.array(range(neval)), Y_best, '-o')\n",
    "plt.hlines(true_min_value, 0, neval, 'k', '--')\n",
    "plt.title('Value of the best candidate')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Best y')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
